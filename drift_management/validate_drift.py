#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¦‚å¿µæ¼‚ç§»éªŒè¯è„šæœ¬
éªŒè¯ç”Ÿæˆçš„æ•°æ®æ˜¯å¦çœŸçš„åŒ…å«æ¦‚å¿µæ¼‚ç§»
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import os
import warnings
warnings.filterwarnings('ignore')

def validate_drift_detection(csv_path, drift_type, window_size=100):
    """
    éªŒè¯æ¦‚å¿µæ¼‚ç§»æ˜¯å¦è¢«æˆåŠŸæ³¨å…¥
    é€šè¿‡åˆ†ææ•°æ®åˆ†å¸ƒå’Œåˆ†ç±»å™¨æ€§èƒ½å˜åŒ–æ¥æ£€æµ‹æ¼‚ç§»
    """
    print(f"ğŸ“Š éªŒè¯ {drift_type} æ¼‚ç§»æ•°æ®: {os.path.basename(csv_path)}")
    
    # è¯»å–æ•°æ®
    df = pd.read_csv(csv_path)
    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values
    
    print(f"   æ•°æ®é›†å½¢çŠ¶: {X.shape}")
    print(f"   ç±»åˆ«æ•°: {len(np.unique(y))}")
    
    # è®¡ç®—æ»‘åŠ¨çª—å£çš„ç»Ÿè®¡ä¿¡æ¯
    n_samples = len(X)
    n_windows = n_samples // window_size
    
    # å­˜å‚¨æ¯ä¸ªçª—å£çš„ç»Ÿè®¡ä¿¡æ¯
    window_means = []
    window_stds = []
    window_accuracies = []
    window_labels = []
    
    # è®­ç»ƒåˆå§‹åˆ†ç±»å™¨
    scaler = StandardScaler()
    clf = LogisticRegression(random_state=42, max_iter=1000)
    
    # ä½¿ç”¨å‰ä¸¤ä¸ªçª—å£è®­ç»ƒåˆ†ç±»å™¨
    if n_windows >= 2:
        train_X = X[:2*window_size]
        train_y = y[:2*window_size]
        train_X_scaled = scaler.fit_transform(train_X)
        clf.fit(train_X_scaled, train_y)
    
    # åˆ†ææ¯ä¸ªçª—å£
    for i in range(n_windows):
        start_idx = i * window_size
        end_idx = min((i + 1) * window_size, n_samples)
        
        window_X = X[start_idx:end_idx]
        window_y = y[start_idx:end_idx]
        
        # è®¡ç®—ç‰¹å¾ç»Ÿè®¡
        mean_features = np.mean(window_X, axis=0)
        std_features = np.std(window_X, axis=0)
        
        window_means.append(np.mean(mean_features))
        window_stds.append(np.mean(std_features))
        
        # è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡
        if len(window_X) > 0:
            window_X_scaled = scaler.transform(window_X)
            y_pred = clf.predict(window_X_scaled)
            accuracy = accuracy_score(window_y, y_pred)
            window_accuracies.append(accuracy)
        else:
            window_accuracies.append(0.0)
        
        # è®¡ç®—æ ‡ç­¾åˆ†å¸ƒ
        label_dist = np.bincount(window_y.astype(int))
        window_labels.append(label_dist)
    
    # åˆ†æç»“æœ
    print(f"   åˆ†æäº† {n_windows} ä¸ªçª—å£")
    print(f"   å¹³å‡ç‰¹å¾å‡å€¼å˜åŒ–: {np.std(window_means):.4f}")
    print(f"   å¹³å‡ç‰¹å¾æ ‡å‡†å·®å˜åŒ–: {np.std(window_stds):.4f}")
    print(f"   å‡†ç¡®ç‡å˜åŒ–: {np.std(window_accuracies):.4f}")
    print(f"   æœ€ä½å‡†ç¡®ç‡: {np.min(window_accuracies):.4f}")
    print(f"   æœ€é«˜å‡†ç¡®ç‡: {np.max(window_accuracies):.4f}")
    
    # æ£€æµ‹æ¼‚ç§»ç‚¹
    drift_detected = False
    if len(window_accuracies) > 2:
        # æ£€æŸ¥å‡†ç¡®ç‡æ˜¯å¦æœ‰æ˜¾è‘—ä¸‹é™
        accuracy_changes = np.diff(window_accuracies)
        significant_drops = np.where(accuracy_changes < -0.1)[0]
        
        if len(significant_drops) > 0:
            drift_detected = True
            print(f"   ğŸ” æ£€æµ‹åˆ°æ¼‚ç§»ç‚¹: çª—å£ {significant_drops}")
    
    # ç‰¹å¾å˜åŒ–æ£€æµ‹
    if len(window_means) > 2:
        mean_changes = np.diff(window_means)
        significant_mean_changes = np.where(np.abs(mean_changes) > np.std(mean_changes) * 2)[0]
        
        if len(significant_mean_changes) > 0:
            drift_detected = True
            print(f"   ğŸ” æ£€æµ‹åˆ°ç‰¹å¾åˆ†å¸ƒå˜åŒ–: çª—å£ {significant_mean_changes}")
    
    if drift_detected:
        print(f"   âœ… æˆåŠŸæ£€æµ‹åˆ°æ¦‚å¿µæ¼‚ç§»ï¼")
    else:
        print(f"   âš ï¸  æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æ¦‚å¿µæ¼‚ç§»")
    
    return {
        'drift_type': drift_type,
        'window_means': window_means,
        'window_stds': window_stds,
        'window_accuracies': window_accuracies,
        'drift_detected': drift_detected,
        'n_windows': n_windows
    }

def compare_with_original(original_dataset='Waveform', n_samples=3000):
    """
    ä¸åŸå§‹æ•°æ®é›†è¿›è¡Œæ¯”è¾ƒ
    """
    print(f"\nğŸ”„ ä¸åŸå§‹ {original_dataset} æ•°æ®é›†æ¯”è¾ƒ...")
    
    # ç”ŸæˆåŸå§‹æ•°æ®
    from skmultiflow.data import WaveformGenerator
    original_stream = WaveformGenerator(random_state=42)
    X_original, y_original = original_stream.next_sample(n_samples)
    
    # è®¡ç®—åŸå§‹æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
    original_mean = np.mean(X_original.flatten())
    original_std = np.std(X_original.flatten())
    
    print(f"   åŸå§‹æ•°æ® - å‡å€¼: {original_mean:.4f}, æ ‡å‡†å·®: {original_std:.4f}")
    
    # æ¯”è¾ƒæ¼‚ç§»æ•°æ®
    drift_files = [
        'Waveform_sudden_severity0.3_pos0.5_width0.1.csv',
        'Waveform_gradual_severity0.5_pos0.5_width0.2.csv',
        'Waveform_incremental_severity0.4_pos0.3_width0.1.csv',
        'Waveform_recurring_severity0.3_pos0.5_width0.1.csv'
    ]
    
    for filename in drift_files:
        filepath = os.path.join('drift_datasets', filename)
        if os.path.exists(filepath):
            df = pd.read_csv(filepath)
            X_drift = df.iloc[:, :-1].values
            
            drift_mean = np.mean(X_drift.flatten())
            drift_std = np.std(X_drift.flatten())
            
            print(f"   {filename.split('_')[1]:12} - å‡å€¼: {drift_mean:.4f}, æ ‡å‡†å·®: {drift_std:.4f}")
            print(f"   {'':12}   å‡å€¼å·®å¼‚: {abs(drift_mean - original_mean):.4f}, æ ‡å‡†å·®å·®å¼‚: {abs(drift_std - original_std):.4f}")

def main():
    """ä¸»éªŒè¯å‡½æ•°"""
    print("ğŸ” æ¦‚å¿µæ¼‚ç§»éªŒè¯å·¥å…·")
    print("=" * 50)
    
    # éªŒè¯æ‰€æœ‰ç”Ÿæˆçš„æ¼‚ç§»æ•°æ®
    drift_files = [
        ('Waveform_sudden_severity0.3_pos0.5_width0.1.csv', 'sudden'),
        ('Waveform_gradual_severity0.5_pos0.5_width0.2.csv', 'gradual'),
        ('Waveform_incremental_severity0.4_pos0.3_width0.1.csv', 'incremental'),
        ('Waveform_recurring_severity0.3_pos0.5_width0.1.csv', 'recurring')
    ]
    
    results = []
    
    for filename, drift_type in drift_files:
        filepath = os.path.join('drift_datasets', filename)
        if os.path.exists(filepath):
            result = validate_drift_detection(filepath, drift_type)
            results.append(result)
            print()
    
    # ä¸åŸå§‹æ•°æ®æ¯”è¾ƒ
    compare_with_original()
    
    # æ€»ç»“æŠ¥å‘Š
    print(f"\nğŸ‰ éªŒè¯æ€»ç»“:")
    successful_detections = sum(1 for r in results if r['drift_detected'])
    total_tests = len(results)
    
    print(f"   æˆåŠŸæ£€æµ‹åˆ°æ¼‚ç§»: {successful_detections}/{total_tests}")
    
    for result in results:
        status = "âœ… æ£€æµ‹åˆ°" if result['drift_detected'] else "âŒ æœªæ£€æµ‹åˆ°"
        print(f"   {result['drift_type']:12} | {status}")
    
    if successful_detections == total_tests:
        print("\nğŸ† æ‰€æœ‰æ¼‚ç§»éƒ½è¢«æˆåŠŸæ£€æµ‹åˆ°ï¼æ¼‚ç§»æ³¨å…¥å·¥å…·å·¥ä½œæ­£å¸¸ã€‚")
    elif successful_detections > 0:
        print(f"\nâš ï¸  éƒ¨åˆ†æ¼‚ç§»è¢«æ£€æµ‹åˆ°ã€‚å¯èƒ½éœ€è¦è°ƒæ•´æ¼‚ç§»å‚æ•°ã€‚")
    else:
        print(f"\nâŒ æœªæ£€æµ‹åˆ°ä»»ä½•æ¼‚ç§»ã€‚è¯·æ£€æŸ¥æ¼‚ç§»æ³¨å…¥é€»è¾‘ã€‚")

if __name__ == "__main__":
    main() 